{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 implementation in Jupyter Notebook\n",
    "based on https://jaykmody.com/blog/gpt-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at first we need an encoder, hyperparameters and pretrained weights for our gpt-2 implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_encoder_hparams_and_params\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(\"124M\", \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# GELU activation function (look https://arxiv.org/pdf/1606.08415.pdf for more information)\n",
    "def gelu(x): \n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Numerically stable version of SoftMax function (copied from https://jaykmody.com/blog/stable-softmax/)\n",
    "def softmax(x): \n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# For GPT-2 architecture we need specific function for normalization with scale and offset effects based on params\n",
    "def layer_norm(x, g, b, eps: float = 1e-5): \n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
    "    return g * x + b  # scale and offset with gamma/beta params\n",
    "\n",
    "# Linear Projection Function: standard matrix multiplication + bias\n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b\n",
    "\n",
    "# Feed Forward Network - perceptron with 2 layers (GELU + projection from n_embd dimension to a higher dimension 4*n_embd and then back down to n_embd\n",
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # GELU + project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "# Mask generation function - to prevent all the queries in our input from looking into the future, we need matrix sustainable to modify our attention matrix to hide future tokens\n",
    "def get_mask(n_seq):\n",
    "    return (1 - np.tri(n_seq)) * -1e10 # for n_seq*n_seq attention matrix we will get n_seq*n_seq mask matrix filled with 0 and -1e10\n",
    "\n",
    "# Attention function (from https://jaykmody.com/blog/attention-intuition/)\n",
    "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "    # assumes q is a matrix of shape (n_q, d_k)\n",
    "    # assumes k is a matrix of shape (n_k, d_k)\n",
    "    # assumes v is a matrix of shape (n_k, d_v)\n",
    "    # output is a matrix of shape (n_q, d_v)\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "\n",
    "# Multi-Head Self Attention implementation - we are performing n_head separate attention computations, splitting our queries, keys, and values into heads and on each head we perform such steps: linear projections, applying mask, letting our input sequence attend to itself, bias vectors addition and out projection \n",
    "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # qkv(querry,key,value) linear projections\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    causal_mask =  get_mask(x.shape[0])  # [n_seq, n_seq]\n",
    "\n",
    "    # perform causal self attention over each head\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # merge heads\n",
    "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection + bias\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "# Transformer decoder block wich consists of two sublayers\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "# GPT-2 architecture putted together\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "    # x[i] represents the word embedding for the ith word + the positional\n",
    "    # embedding for the ith position\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # LM head\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
    "\n",
    "# Function for generation of output based on prompt\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|██████████| 25/25 [00:19<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: For millennia, the main use of ferrets was for hunting, or \"ferreting\". With their long, lean build, and inquisitive nature, ferrets are very well equipped for getting down holes and chasing rodents, rabbits and moles out of their burrows.\n",
      "GPT-2: \n",
      "\n",
      "Ferrets are also very good at hunting small mammals, such as rabbits, and are very good at hunting large mammals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# so we need some prompt to test our GPT-2\n",
    "prompt = 'For millennia, the main use of ferrets was for hunting, or \\\"ferreting\\\". With their long, lean build, and inquisitive nature, ferrets are very well equipped for getting down holes and chasing rodents, rabbits and moles out of their burrows.'\n",
    "\n",
    "# and we need to specify amount of tokens we want GPT to generate\n",
    "n_tokens_to_generate = 25\n",
    "\n",
    "# encode the input string using the encoder (BPE tokenizer)\n",
    "input_ids = encoder.encode(prompt)\n",
    "\n",
    "# make sure we are not surpassing the max sequence length of our model\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "# decode the ids back into a string\n",
    "output_text = encoder.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prompt:\n",
      "For millennia, the main use of ferrets was for hunting, or \"ferreting\". With their long, lean build, and inquisitive nature, ferrets are very well equipped for getting down holes and chasing rodents, rabbits and moles out of their burrows.\n",
      "\n",
      "GPT-2:\n",
      "\n",
      "\n",
      "Ferrets are also very good at hunting small mammals, such as rabbits, and are very good at hunting large mammals\n"
     ]
    }
   ],
   "source": [
    "# lets see what we got\n",
    "print('\\nprompt:\\n' + prompt)\n",
    "print('\\nGPT-2:\\n' + output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFerrets are also very good at hunting small mammals, such as rabbits, and are very good at hunting large mammals'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
